debug: no
no_cuda: no
epochs_no: 100
batch_size: 1024
test_batch_size: 10000

task_model:
  name: MLP
  units: [100]
  use_bias: False
  use_dropout: False

task_optimizer:
  name: Adam
  lr: 0.001
  nesterov: delete

student_optimizers:
  - name: SGD
    lr: 0.01
  - name: SGD
    lr: 0.001
  - name: Adam
    lr: 0.0001

l2: 0.2

loss_learning:
  nprofessors: 5
  loss_predictor:
    name: CorrLossPredictor
    inter_layer: yes
    width: 3
  optimizer:
    name: Adam
    lr: .001
    nesterov: delete
  param_samples: 12
  noise_intensity: .01
  noise_type: absolute
  full_grad: yes
  sobolev_depth: 2
  optim_loss: yes
  c0: 1.0
  c1: 1.0
  c2: 1.0
  ccos: 10.0
  coptim: 1.0

evaluation:
  random_params: no
  teaching_steps: 2000
  teaching_eval_freq: 100


log_interval: 60000
eval_interval: 600000
